{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "from IPython.lib.display import YouTubeVideo\n",
    "from IPython.display import Image\n",
    "\n",
    "def css_styling():\n",
    "    styles = open(\"custom.css\", \"r\").read()\n",
    "    return HTML(styles)\n",
    "css_styling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Code in Python is 'commented out' when it is preceded by a hash sign. \n",
    "# Use comments to make your code more readable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A brief demo of k-means clustering\n",
    "### Written by Reese Richardson for use in Biol Sci 378, Winter 2022, Northwestern University (rakr@u.northwestern.edu)\n",
    "\n",
    "How does k-means clustering work? Let's find out! First, we'll install some packages we'll need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my favorite packages\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "# things we'll need for k-means specifically\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The procedure for k-means is actually quite simple!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='k_means_procedure.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some synthetic data using the `make_blobs` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, true_labels = make_blobs(n_samples=200, # generate 200 data points\n",
    "                                   n_features=2, # with 2 coordinates each\n",
    "                                   centers=8, # around 8 true cluster centers\n",
    "                                   cluster_std=2, # with a standard deviation of 2\n",
    "                                   random_state=10) # random seed (try changing it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "axes[1].scatter(features[:,0], features[:,1], s=50, alpha=0.75, linewidth=0)\n",
    "\n",
    "for label in np.unique(true_labels):\n",
    "    axes[0].scatter(features[:,0][true_labels == label], \n",
    "                features[:,1][true_labels == label], \n",
    "                s=50, alpha=0.75, linewidth=0)\n",
    "    \n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('square')\n",
    "    ax.tick_params(labelsize=14)\n",
    "    ax.set_xlabel('x', fontsize=14)\n",
    "    ax.set_ylabel('y', fontsize=14, rotation=0)\n",
    "    \n",
    "axes[1].set_title('without labels', fontsize=16)\n",
    "axes[0].set_title('with true identity labeled', fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the true cluster of origin of each point labeled, it is easy to see how the clusters relate. However, we often don't know the true underlying structure of the relationships between these points! Our view is normally more similar to that on the right plot above than on the left. Let's use `StandardScaler` to z-score normalize both our dimensions here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # instantiate StandardScaler\n",
    "scaled_features = scaler.fit_transform(features) # get scaled features\n",
    "scaled_features[0:5,] # show first five points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "axes[1].scatter(scaled_features[:,0], scaled_features[:,1], s=50, alpha=0.75, linewidth=0)\n",
    "\n",
    "for label in np.unique(true_labels):\n",
    "    axes[0].scatter(scaled_features[:,0][true_labels == label], \n",
    "                scaled_features[:,1][true_labels == label], \n",
    "                s=50, alpha=0.75, linewidth=0)\n",
    "    \n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('square')\n",
    "    ax.tick_params(labelsize=14)\n",
    "    ax.set_xlabel('x', fontsize=14)\n",
    "    ax.set_ylabel('y', fontsize=14, rotation=0)\n",
    "    \n",
    "axes[1].set_title('without labels', fontsize=16)\n",
    "axes[0].set_title('with true identity labeled', fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This weights all dimensions (for us, just x and y) of our dataset equally so that no dimensions contributes more to our error function than any other. Next, we will instantiate an Object of `KMeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 8\n",
    "kmeans = KMeans(init=\"random\", # instantiate KMeans\n",
    "                n_clusters=k, # what k to use (k clusters)\n",
    "                n_init=1, # number of times to initiate (you can initate multiple times to find a more optimal solution)\n",
    "                max_iter=25, # number of iterations of expectation/maximization cycle to run through\n",
    "                tol=0, # you can end cycling of expectation/maximization early if you meet a certain tolerance threshold\n",
    "                # this is called \"declaring convergence\"\n",
    "                # we will set this tolerance threshold to zero so that we go through every iteration.\n",
    "                random_state=42) # random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use this instance of `kmean` to peform clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(scaled_features) # perform k-means clustering\n",
    "print('Locations of cluster centers: ')\n",
    "print(kmeans.cluster_centers_)\n",
    "print('\\nCluster identities of each point at the end of iterations (predicted labels): ')\n",
    "print(kmeans.labels_)\n",
    "print('\\nSum of square errors (SSE; sum of the squares of distances of each point to its cluster centriod): ')\n",
    "print(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "for label in np.unique(kmeans.labels_):\n",
    "    axes[0].scatter(scaled_features[:,0][kmeans.labels_ == label], \n",
    "                scaled_features[:,1][kmeans.labels_ == label], \n",
    "                s=50, alpha=0.75, linewidth=0, color=sns.color_palette()[label])\n",
    "    axes[0].scatter(*kmeans.cluster_centers_[label], \n",
    "                    color=sns.color_palette()[label], \n",
    "                    marker='*', s=200, linewidth=1, edgecolor='k')\n",
    "\n",
    "for label in np.unique(true_labels):\n",
    "    axes[1].scatter(scaled_features[:,0][true_labels == label], \n",
    "                scaled_features[:,1][true_labels == label], \n",
    "                s=30, alpha=0.75, linewidth=0, color=sns.color_palette()[label])\n",
    "\n",
    "axes[0].scatter([], [], color=sns.color_palette()[label], marker='*', \n",
    "                s=200, linewidth=1, edgecolor='k', label='centroids')\n",
    "axes[0].legend(fontsize=14)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('square')\n",
    "    ax.tick_params(labelsize=14)\n",
    "    ax.set_xlabel('x', fontsize=14)\n",
    "    ax.set_ylabel('y', fontsize=14, rotation=0)\n",
    "    \n",
    "axes[0].set_title('predicted labels (k = ' + str(k) + ')', fontsize=16)\n",
    "axes[1].set_title('true labels', fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize how the SSE changes with each successive iteration,up to 50 for this toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 50\n",
    "k = 8\n",
    "sse_array = []\n",
    "for n in range(1,n_iter+1):\n",
    "    kmeans = KMeans(init=\"random\", # instantiate KMeans\n",
    "                    n_clusters=k, # with k = 5 (5 clusters)\n",
    "                    n_init=1, # number of times to initiate (you can initate multiple times to find a more optimal solution)\n",
    "                    max_iter=n, # number of iterations of expectation/maximization cycle to run through\n",
    "                    tol=0, # you can end cycling of expectation/maximization early if you meet a certain tolerance threshold\n",
    "                    # this is called \"declaring convergence\"\n",
    "                    # we will set this tolerance threshold to zero so that we go through every iteration.\n",
    "                    random_state=42) # random seed\n",
    "    kmeans.fit(scaled_features)\n",
    "    sse_array.append(kmeans.inertia_)\n",
    "sse_array = np.array(sse_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,3))\n",
    "plt.plot(range(1,n_iter+1),sse_array, linewidth=2)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.set_xlabel('number of iterations', fontsize=16)\n",
    "ax.set_ylabel('SSE', fontsize=16, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, SSE approaches its minimum value quite quickly. You can actually set the parameter `tol` to a nonzero value if you would like to stop after the difference between the previous iteration's SSE and this iteration's SSE falls below a particular threshold. Default `tol = 1e-4`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the big question: if you don't know the underlying cluster structure of your data...\n",
    "## ... how do you know which value of *k* to select?\n",
    "\n",
    "Let's see what the SSE value converges to after 50 iterations with multiple values of `k`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 50\n",
    "k_range = np.arange(2,31)\n",
    "sse_array = []\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(init=\"random\", # instantiate KMeans\n",
    "                    n_clusters=k, # with k = 5 (5 clusters)\n",
    "                    n_init=1, # number of times to initiate (you can initate multiple times to find a more optimal solution)\n",
    "                    max_iter=n_iter, # number of iterations of expectation/maximization cycle to run through\n",
    "                    tol=0, # you can end cycling of expectation/maximization early if you meet a certain tolerance threshold\n",
    "                    # this is called \"declaring convergence\"\n",
    "                    # we will set this tolerance threshold to zero so that we go through every iteration.\n",
    "                    random_state=42) # random seed\n",
    "    kmeans.fit(scaled_features)\n",
    "    sse_array.append(kmeans.inertia_)\n",
    "sse_array = np.array(sse_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,3))\n",
    "plt.plot(k_range,sse_array, linewidth=2)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.set_xlabel('k', fontsize=16)\n",
    "ax.set_ylabel('SSE', fontsize=16, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As `k` increases, our SSE first goes down by a lot with each successive `k`, then by a little. This makes sense, since having more clusters means each data point will naturally be closer to its cluster centroid. To find an optimal value of `k`, many data scientists will select where the \"knee\" of this curve. This is the location where adding an additional cluster provides little additional marginal benefit for SSE. \n",
    "\n",
    "There isn't a solid quantitative definition of where this is, and the knee location can change drastically depending on which definition you choose. Thus I'm not very fond of using this method and would never use in a real-world application. However, if you were to compel me, under great duress, to select a knee on this graph, I would probably put it here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,3))\n",
    "plt.plot(k_range,sse_array, linewidth=2)\n",
    "plt.axvline(6, linewidth=2, color='r', linestyle='dashed')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.set_xlabel('k', fontsize=16)\n",
    "ax.set_ylabel('SSE', fontsize=16, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why at `k = 6`? I don't know. Why are you forcing me to do this? That's real weird of you, not gonna lie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are looking for a more quantitative approach, you might select the value of `k` that maximizes the [silhouette score](https://en.wikipedia.org/wiki/Silhouette_(clustering)), a metric that compares distances between points within a cluster to distances between points between clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 50\n",
    "k_range = np.arange(2,31)\n",
    "silhouette_array = []\n",
    "for k in k_range:\n",
    "    kmeans = KMeans(init=\"random\", # instantiate KMeans\n",
    "                    n_clusters=k, # with k = 5 (5 clusters)\n",
    "                    n_init=1, # number of times to initiate (you can initate multiple times to find a more optimal solution)\n",
    "                    max_iter=n_iter, # number of iterations of expectation/maximization cycle to run through\n",
    "                    tol=0, # you can end cycling of expectation/maximization early if you meet a certain tolerance threshold\n",
    "                    # this is called \"declaring convergence\"\n",
    "                    # we will set this tolerance threshold to zero so that we go through every iteration.\n",
    "                    random_state=42) # random seed\n",
    "    kmeans.fit(scaled_features)\n",
    "    silhouette_array.append(silhouette_score(scaled_features, kmeans.labels_))\n",
    "silhouette_array = np.array(silhouette_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,3))\n",
    "plt.plot(k_range,silhouette_array, linewidth=2)\n",
    "plt.axvline(k_range[np.argmax(silhouette_array)], linewidth=2, color='r', linestyle='dashed')\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.tick_params(labelsize=14)\n",
    "ax.set_xlabel('k', fontsize=16)\n",
    "ax.set_ylabel('silhouette coefficient', fontsize=16, rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This returns `k = 4` for this data. Let's cluster one last time with this `k` and set a nonzero value for `tol` so that we end iterating once we have found a pretty good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 4\n",
    "kmeans = KMeans(init=\"random\", # instantiate KMeans\n",
    "                n_clusters=k, # what k to use (k clusters)\n",
    "                n_init=1, # number of times to initiate (you can initate multiple times to find a more optimal solution)\n",
    "                max_iter=300, # maximum number of iterations of expectation/maximization cycle to run through\n",
    "                tol=, # you can end cycling of expectation/maximization early if you meet a certain tolerance threshold\n",
    "                    # this is called \"declaring convergence\"\n",
    "                random_state=42) # random seed\n",
    "kmeans.fit(scaled_features)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,5))\n",
    "\n",
    "for label in np.unique(kmeans.labels_):\n",
    "    axes[0].scatter(scaled_features[:,0][kmeans.labels_ == label], \n",
    "                scaled_features[:,1][kmeans.labels_ == label], \n",
    "                s=50, alpha=0.75, linewidth=0, color=sns.color_palette()[label])\n",
    "    axes[0].scatter(*kmeans.cluster_centers_[label], \n",
    "                    color=sns.color_palette()[label], \n",
    "                    marker='*', s=200, linewidth=1, edgecolor='k')\n",
    "\n",
    "for label in np.unique(true_labels):\n",
    "    axes[1].scatter(scaled_features[:,0][true_labels == label], \n",
    "                scaled_features[:,1][true_labels == label], \n",
    "                s=50, alpha=0.75, linewidth=0, color=sns.color_palette()[label])\n",
    "\n",
    "axes[0].scatter([], [], color=sns.color_palette()[label], marker='*', \n",
    "                s=200, linewidth=1, edgecolor='k', label='centroids')\n",
    "axes[0].legend(fontsize=14)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axis('square')\n",
    "    ax.tick_params(labelsize=14)\n",
    "    ax.set_xlabel('x', fontsize=14)\n",
    "    ax.set_ylabel('y', fontsize=14, rotation=0)\n",
    "    \n",
    "axes[0].set_title('predicted labels (k = ' + str(k) + ')', fontsize=16)\n",
    "axes[1].set_title('true labels', fontsize=16)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.n_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does `k = 4` accurately describe the cluster structure from which this data was generated? No, but when you use k-means clustering in genomics, this underlying structure is often unknowable. We really on methods like k-means and hierarchical clustering to make \"good enough\" approximations.\n",
    "\n",
    "Try playing around with the parameters in this notebook to see how clustering changes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
